{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "texas_counties = [\n",
    "    'Anderson', 'Andrews', 'Angelina', 'Aransas', 'Archer', 'Armstrong', 'Atascosa', 'Bailey', 'Bandera', \n",
    "    'Bastrop', 'Baylor', 'Bee', 'Bell', 'Bexar', 'Blanco', 'Borden', 'Bosque', 'Bowie', 'Brazoria', 'Brazos', \n",
    "    'Brewster', 'Briscoe', 'Brooks', 'Brown', 'Burleson', 'Burnet', 'Caldwell', 'Calhoun', 'Callahan', 'Cameron', \n",
    "    'Camp', 'Carson', 'Cass', 'Castro', 'Chambers', 'Cherokee', 'Childress', 'Clay', 'Cochran', 'Coke', 'Coleman', \n",
    "    'Collin', 'Collingsworth', 'Colorado', 'Comal', 'Comanche', 'Concho', 'Cooke', 'Coryell', 'Cottle', 'Crane', \n",
    "    'Crockett', 'Crosby', 'Culberson', 'Dallam', 'Dallas', 'Dawson', 'Deaf Smith', 'Delta', 'Denton', 'DeWitt', \n",
    "    'Dickens', 'Dimmit', 'Donley', 'Duval', 'Eastland', 'Ector', 'Edwards', 'Ellis', 'Erath', 'Falls', \n",
    "    'Fannin', 'Fayette', 'Fisher', 'Floyd', 'Foard', 'Fort Bend', 'Franklin', 'Freestone', 'Frio', 'Gaines', \n",
    "    'Galveston', 'Garza', 'Gillespie', 'Glasscock', 'Goliad', 'Gonzales', 'Gray', 'Grayson', 'Gregg', 'Grimes', \n",
    "    'Guadalupe', 'Hale', 'Hall', 'Hamilton', 'Hansford', 'Hardeman', 'Hardin', 'Harris', 'Harrison', 'Hartley', \n",
    "    'Haskell', 'Hays', 'Hemphill', 'Henderson', 'Hidalgo', 'Hill', 'Hockley', 'Hood', 'Hopkins', 'Howard', \n",
    "    'Hudspeth', 'Hunt', 'Hutchinson', 'Irion', 'Jack', 'Jackson', 'Jasper', 'Jeff Davis', 'Jefferson', 'Jim Hogg', \n",
    "    'Jim Wells', 'Johnson', 'Jones', 'Karnes', 'Kaufman', 'Kendall', 'Kenedy', 'Kent', 'Kerr', 'Kimble', 'King', \n",
    "    'Kinney', 'Kleberg', 'Knox', 'Lamar', 'Lamb', 'Lampasas', 'La Salle', 'Lavaca', 'Lee', 'Leon', 'Liberty', \n",
    "    'Limestone', 'Lipscomb', 'Live Oak', 'Llano', 'Loving', 'Lynn', 'McCulloch', 'McLennan', 'McMullen', \n",
    "    'Madison', 'Marion', 'Martin', 'Mason', 'Matagorda', 'Maverick', 'Medina', 'Menard', 'Midland', 'Milam', \n",
    "    'Mills', 'Mitchell', 'Montague', 'Montgomery', 'Moore', 'Morris', 'Motley', 'Nacogdoches', 'Navarro', \n",
    "    'Newton', 'Nolan', 'Nueces', 'Ochiltree', 'Oldham', 'Orange', 'Palo Pinto', 'Panola', 'Parker', 'Parmer', \n",
    "    'Pecos', 'Polk', 'Potter', 'Presidio', 'Rains', 'Randall', 'Reagan', 'Real', 'Red River', 'Reeves', 'Refugio', \n",
    "    'Roberts', 'Robertson', 'Rockwall', 'Runnels', 'Rusk', 'Sabine', 'San Augustine', 'San Jacinto', 'San Patricio', \n",
    "    'San Saba', 'Schleicher', 'Scurry', 'Shackelford', 'Shelby', 'Sherman', 'Smith', 'Somervell', 'Starr', \n",
    "    'Stephens', 'Sterling', 'Stonewall', 'Sutton', 'Swisher', 'Tarrant', 'Taylor', 'Terrell', 'Terry', 'Throckmorton', \n",
    "    'Titus', 'Tom Green', 'Travis', 'Trinity', 'Tyler', 'Upshur', 'Upton', 'Uvalde', 'Val Verde', 'Van Zandt', \n",
    "    'Victoria', 'Walker', 'Waller', 'Ward', 'Washington', 'Webb', 'Wharton', 'Wheeler', 'Wichita', 'Wilbarger', \n",
    "    'Willacy', 'Williamson', 'Wilson', 'Winkler', 'Wise', 'Wood', 'Yoakum', 'Young', 'Zapata', 'Zavala'\n",
    "]\n",
    "\n",
    "texas_counties_df = pl.DataFrame({\"county\": texas_counties})\n",
    "texas_counties_df.write_csv('texas_counties.csv')\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_counties_df = texas_counties_df.with_columns( pl.col(\"county\").str.to_uppercase())\n",
    "texas_counties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "ROUTES_QUERY = \"\"\"--sql \n",
    "    SELECT route_name FROM properties_dba.routes\n",
    "\"\"\"\n",
    "\n",
    "routes_df = await PG.fetch(\"SELECT route_id, route_name, division_id FROM properties_dba.routes\")\n",
    "if routes_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if routes_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "routes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "FACILITIES_QUERY = \"\"\"--sql \n",
    "    SELECT *\n",
    "    FROM properties_dba.mrte_facility\n",
    "    WHERE primo_prprty NOT IN (69419, 98750, 98743);\n",
    "\"\"\"\n",
    "\n",
    "facilities_df = await PG.fetch(FACILITIES_QUERY)\n",
    "if facilities_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if facilities_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "facilities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_route_to_division(route_df: pl.DataFrame, division_id: int) -> str:\n",
    "    get_divsion = route_df.filter(pl.col(\"division_id\") == division_id)\n",
    "    route_assignment = random.choice(get_divsion)\n",
    "    route_chosen = route_assignment.select(\"route_id\").item()\n",
    "    return route_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_route_to_division(routes_df, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facility gen\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def map_route_to_division(route_df: pl.DataFrame, division_id: int):\n",
    "    get_divsion = route_df.filter(pl.col(\"division_id\") == division_id)\n",
    "    route_assignment = random.choice(get_divsion)\n",
    "    route_chosen = route_assignment.select(\"route_id\").item()\n",
    "    return route_chosen\n",
    "\n",
    "def testing_data() -> pl.DataFrame:\n",
    "    county_row_count = texas_counties_df.shape[0]\n",
    "    \n",
    "    facilities = []\n",
    "\n",
    "    facility_id = 348823750\n",
    "    primo_prprty = 98760\n",
    "\n",
    "    batch_size = 10\n",
    "    start_index = 0\n",
    "\n",
    "    for route in routes_df.iter_rows(named=True):\n",
    "        division_id = route['division_id']\n",
    "\n",
    "        # Generate 10 facilities for each route\n",
    "        for i in range(1, 11):\n",
    "            # Ensure start_index wraps around if it exceeds county_count\n",
    "            county_index = (start_index + i - 1) % county_row_count\n",
    "            county_name = texas_counties_df[\"county\"][county_index]\n",
    "\n",
    "            facility_id += 1\n",
    "            primo_prprty +=1\n",
    "            route_id = map_route_to_division(routes_df, division_id)\n",
    "\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=365 * 10)\n",
    "            first_production_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "            facilities.append({\n",
    "                'facility_id': facility_id,\n",
    "                'facility_name': f'{county_name} FAC',\n",
    "                'first_production_ts': first_production_ts,    \n",
    "                'division_id': division_id,\n",
    "                'primo_prprty': primo_prprty,\n",
    "                'route_id': route_id\n",
    "            })\n",
    "\n",
    "        start_index = (start_index + batch_size) % county_row_count\n",
    "\n",
    "\n",
    "    facilities_df = pl.DataFrame(facilities)\n",
    "    return facilities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "facilities_df = facilities_df.with_columns(pl.col(\"primo_prprty\").cast(str))\n",
    "facilities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tank gen\n",
    "import random\n",
    "\n",
    "import uuid\n",
    "\n",
    "\n",
    "def tank_generation() -> pl.DataFrame:\n",
    "    tanks = []\n",
    "\n",
    "    for facs in facilities_df.iter_rows(named=True):\n",
    "        division_id = facs['division_id']\n",
    "        primo_prprty =  facs['primo_prprty']\n",
    "\n",
    "        range_oil_tanks = random.randint(2,6)\n",
    "        range_water_tanks = random.randint(1,3)\n",
    "\n",
    "\n",
    "        #generating oil tanks\n",
    "        for i in range(1, range_oil_tanks + 1):\n",
    "            tanks.append({\n",
    "                'key_metric': uuid.uuid4(),\n",
    "                'source_key': primo_prprty + '0' + str(i),\n",
    "                'source_id': primo_prprty,\n",
    "                'metric_nice_name': 'OilTank' + str(i) + 'Level',\n",
    "                'uom': 'in',\n",
    "                'division_id': division_id\n",
    "            })\n",
    "            tanks.append({\n",
    "                'key_metric': uuid.uuid4(),\n",
    "                'source_key': primo_prprty + '0' + str(i),\n",
    "                'source_id': primo_prprty,\n",
    "                'metric_nice_name': 'OilTank' + str(i) + 'Volume',\n",
    "                'uom': 'bbl',\n",
    "                'division_id': division_id\n",
    "            })\n",
    "\n",
    "        # generating water tanks\n",
    "        for i in range(1, range_water_tanks + 1):\n",
    "            tanks.append({\n",
    "                'key_metric': uuid.uuid4(),\n",
    "                'source_key': primo_prprty + 'W' + str(i),\n",
    "                'source_id': primo_prprty,\n",
    "                'metric_nice_name': 'WaterTank' + str(i) + 'Level',\n",
    "                'uom': 'in',\n",
    "                'division_id': division_id\n",
    "            })\n",
    "            tanks.append({\n",
    "                'key_metric': uuid.uuid4(),\n",
    "                'source_key': primo_prprty + 'W' + str(i),\n",
    "                'source_id': primo_prprty,\n",
    "                'metric_nice_name': 'WaterTank' + str(i) + 'Volume',\n",
    "                'uom': 'bbl',\n",
    "                'division_id': division_id\n",
    "            })\n",
    "\n",
    "        tanks.append({\n",
    "                'key_metric': uuid.uuid4(),\n",
    "                'source_key': primo_prprty + 'FAC',\n",
    "                'source_id': primo_prprty,\n",
    "                'metric_nice_name': 'ESD-OilTankID',\n",
    "                'uom': '',\n",
    "                'division_id': division_id\n",
    "        })\n",
    "\n",
    "        tanks.append({\n",
    "            'key_metric': uuid.uuid4(),\n",
    "            'source_key': primo_prprty + 'FAC',\n",
    "            'source_id': primo_prprty,\n",
    "            'metric_nice_name': 'ESD-OilTankInchesUntilAlarm',\n",
    "            'uom': 'in',\n",
    "            'division_id': division_id\n",
    "        })\n",
    "\n",
    "        tanks.append({\n",
    "            'key_metric': uuid.uuid4(),\n",
    "            'source_key': primo_prprty + 'FAC',\n",
    "            'source_id': primo_prprty,\n",
    "            'metric_nice_name': 'ESD-WaterTankID',\n",
    "            'uom': '',\n",
    "            'division_id': division_id\n",
    "        })\n",
    "\n",
    "        tanks.append({\n",
    "            'key_metric': uuid.uuid4(),\n",
    "            'source_key': primo_prprty + 'FAC',\n",
    "            'source_id': primo_prprty,\n",
    "            'metric_nice_name': 'ESD-WaterTankInchesUntilAlarm',\n",
    "            'uom': 'in',\n",
    "            'division_id': division_id\n",
    "        })\n",
    "\n",
    "    tanks_df = pl.DataFrame(tanks)\n",
    "    return tanks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "await PG.insert(tank_generation(), \"timeseries_dba.data_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "NEW_TANKS_QUERY = \"\"\"--sql \n",
    "    SELECT DISTINCT source_key, division_id\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "    AND source_key NOT LIKE '%FAC%'\n",
    "    ORDER BY source_key;\n",
    "\"\"\"\n",
    "\n",
    "new_tanks_df = await PG.fetch(NEW_TANKS_QUERY)\n",
    "if new_tanks_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if new_tanks_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "new_tanks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tank_metadata\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from decimal import Decimal\n",
    "\n",
    "def tank_timeseries_generation() -> pl.DataFrame:\n",
    "    tanks_ts = []\n",
    "\n",
    "    for i in new_tanks_df.iter_rows(named=True):\n",
    "        source_key = i[\"source_key\"]\n",
    "        division_id = i[\"division_id\"]\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365 * 5)\n",
    "        create_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "        tanksize = random.uniform(400, 800)\n",
    "\n",
    "        tanks_ts.append({\n",
    "            \"scadaid\": source_key,\n",
    "            \"create_ts\": create_ts, \n",
    "            \"update_ts\": create_ts,\n",
    "            \"tanksize\": tanksize,\n",
    "            \"division_id\": division_id\n",
    "        })\n",
    "\n",
    "    tanks_df = pl.DataFrame(tanks_ts)\n",
    "    return tanks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_timeseries_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "await PG.insert(tank_timeseries_generation(), \"tank.tank_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "DATA_CATALOG_QUERY = \"\"\"--sql \n",
    "    SELECT *\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "\"\"\"\n",
    "\n",
    "dc_df = await PG.fetch(DATA_CATALOG_QUERY)\n",
    "if dc_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if dc_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "dc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "NEW_TANKS_QUERY = \"\"\"--sql \n",
    "    SELECT DISTINCT source_key, division_id, source_id\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "    AND source_key NOT LIKE '%FAC%'\n",
    "    ORDER BY source_key;\n",
    "\"\"\"\n",
    "\n",
    "new_tanks_df = await PG.fetch(NEW_TANKS_QUERY)\n",
    "if new_tanks_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if new_tanks_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "new_tanks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'0\\d'\n",
    "oil_tanks_df = new_tanks_df.filter(pl.col(\"source_key\").str.contains(pattern)) \n",
    "new_oil_df = oil_tanks_df.group_by(\"source_id\").agg(pl.count(\"source_key\").alias(\"oil_tank_count\"))\n",
    "new_oil_df = new_oil_df.sort(\"source_id\")\n",
    "new_oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_tanks_df = new_tanks_df.filter(pl.col(\"source_key\").str.contains(\"W\"))\n",
    "new_df = water_tanks_df.group_by(\"source_id\").agg(pl.count(\"source_key\").alias(\"water_tank_count\"))\n",
    "new_df = new_df.sort(\"source_id\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_count_df = new_df.join(new_oil_df, on=\"source_id\")\n",
    "combined_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_rand_oil(new_df: pl.DataFrame, key_metric):\n",
    "    oil_count = new_df.with_columns(\"oil_tank_count\").item()\n",
    "    return oil_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_df = dc_df.join(combined_count_df, on=\"source_id\")\n",
    "new_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeseries\n",
    "import polars as pl\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def tank_timeseries_with_value() -> pl.DataFrame:\n",
    "    time_series = []\n",
    "    value = 0 \n",
    "\n",
    "    for tank in new_new_df.iter_rows(named=True):\n",
    "        key_metric = tank[\"key_metric\"]\n",
    "        primo_id = tank[\"source_key\"]\n",
    "        identifier = tank[\"metric_nice_name\"]\n",
    "\n",
    "        if \"Inches\" in identifier:\n",
    "            if \"Oil\" in identifier:\n",
    "                value = random.randint(1, tank[\"oil_tank_count\"])\n",
    "            else:\n",
    "                value = random.randint(1, tank[\"water_tank_count\"])\n",
    "        else: continue\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365)\n",
    "        create_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "        time_series.append({\n",
    "            \"key_metric\": key_metric,\n",
    "            \"pp\": primo_id,\n",
    "            \"poop\": identifier,\n",
    "            \"ts\": create_ts,\n",
    "            \"value\": value\n",
    "        })\n",
    "\n",
    "    time_series_df = pl.DataFrame(time_series)\n",
    "    return time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_timeseries_with_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "NEW_TANKS_QUERY = \"\"\"--sql \n",
    "    SELECT *\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "    AND source_key NOT LIKE '%FAC%'\n",
    "    ORDER BY source_key;\n",
    "\"\"\"\n",
    "\n",
    "wee = await PG.fetch(NEW_TANKS_QUERY)\n",
    "if wee is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if wee.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "wee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeseries\n",
    "import polars as pl\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def tank_timeseries_with_value2() -> pl.DataFrame:\n",
    "    time_series = []\n",
    "\n",
    "    for tank in wee.iter_rows(named=True):\n",
    "        key_metric = tank[\"key_metric\"]\n",
    "        identifier = tank[\"metric_nice_name\"]\n",
    "\n",
    "        if \"Volume\" in identifier:\n",
    "            value = random.uniform(120, 480)\n",
    "        elif \"Level\" in identifier:\n",
    "            value = random.uniform(60, 98)\n",
    "        else:\n",
    "            value = random.uniform(60, 400)\n",
    "\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365 * 10)\n",
    "        create_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "        time_series.append({\n",
    "            \"key_metric\": key_metric,\n",
    "            \"ts\": create_ts,\n",
    "            \"value\": value\n",
    "        })\n",
    "\n",
    "    time_series_df = pl.DataFrame(time_series)\n",
    "    return time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_timeseries_with_value2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from perlin_noise import PerlinNoise\n",
    "\n",
    "# Initialize Perlin noise generator\n",
    "random_noise = PerlinNoise(octaves=1)\n",
    "\n",
    "# Function to create the initial tank time series data\n",
    "def tank_timeseries_with_value2() -> pl.DataFrame:\n",
    "    time_series = []\n",
    "\n",
    "    for tank in wee.iter_rows(named=True):\n",
    "        key_metric = tank[\"key_metric\"]\n",
    "        identifier = tank[\"metric_nice_name\"]\n",
    "\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365 * 10)\n",
    "        create_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "        # Generate value using Perlin noise\n",
    "        if \"Volume\" in identifier:\n",
    "            value = 300 + random_noise.noise(create_ts.timestamp() / 100000) * 180\n",
    "        elif \"Level\" in identifier:\n",
    "            value = 79 + random_noise.noise(create_ts.timestamp() / 100000) * 19\n",
    "        else:\n",
    "            value = 230 + random_noise.noise(create_ts.timestamp() / 100000) * 170\n",
    "\n",
    "        time_series.append({\n",
    "            \"key_metric\": key_metric,\n",
    "            \"ts\": create_ts,\n",
    "            \"value\": value\n",
    "        })\n",
    "\n",
    "    time_series_df = pl.DataFrame(time_series)\n",
    "    return time_series_df\n",
    "\n",
    "# Function to apply Perlin noise over time series\n",
    "def create_ts_df_for_min(initial_df: pl.DataFrame, hour: int, total_minutes: int) -> pl.DataFrame:\n",
    "    offset = timedelta(hours=hour)\n",
    "    noise = random_noise.noise(hour / total_minutes)\n",
    "    return initial_df.with_columns(\n",
    "        pl.col(\"key_metric\"),\n",
    "        pl.col(\"ts\") - offset,\n",
    "        pl.col(\"value\") + noise\n",
    "    )\n",
    "\n",
    "# Generate the time series data with Perlin noise\n",
    "points_per_day = 24\n",
    "days = 2\n",
    "hours = points_per_day * days\n",
    "\n",
    "# Use the fetched DataFrame for time series generation\n",
    "initial_df = tank_timeseries_with_value2()\n",
    "noise_dfs = map(\n",
    "    lambda x: create_ts_df_for_min(initial_df, x, hours), range(hours)\n",
    ")\n",
    "noise_dfs = list(noise_dfs)\n",
    "\n",
    "# Combine all generated data into one DataFrame\n",
    "combined_df = pl.concat(noise_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wee.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_df_for_min(initial_df: pl.DataFrame, hour: int, total_minutes: int) -> pl.DataFrame:\n",
    "    offset = timedelta(hours=hour)\n",
    "    noise = random_noise.noise(hour / total_minutes)\n",
    "    return initial_df.with_columns(\n",
    "        pl.col(\"key_metric\"),\n",
    "        pl.col(\"ts\") - offset,\n",
    "        pl.col(\"value\") + noise\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG \n",
    "\n",
    "DATA_CATALOG_QUERY = \"\"\"--sql\n",
    "    SELECT *\n",
    "    FROM timeseries_dba.data_catalog AS dc \n",
    "    WHERE dc.metric_nice_name NOT LIKE '%TankID%'\n",
    "    AND dc.metric_nice_name NOT LIKE '%TimeUntilESD%'\n",
    "\"\"\"\n",
    "\n",
    "dc = await PG.fetch(DATA_CATALOG_QUERY)\n",
    "if dc is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if dc.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facility gen\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def testing_data() -> pl.DataFrame:\n",
    "    \n",
    "    facilities = []\n",
    "\n",
    "    for route in routes_df.iter_rows(named=True):\n",
    "        division_id = route['division_id']\n",
    "\n",
    "        # Generate 10 facilities for each route\n",
    "        for i in range(1, 11):\n",
    "            # Ensure start_index wraps around if it exceeds county_count\n",
    "            county_index = (start_index + i - 1) % county_row_count\n",
    "            county_name = texas_counties_df[\"county\"][county_index]\n",
    "\n",
    "            facility_id += 1\n",
    "            primo_prprty +=1\n",
    "            route_id = map_route_to_division(routes_df, division_id)\n",
    "\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=365 * 10)\n",
    "            first_production_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "            facilities.append({\n",
    "                'facility_id': facility_id,\n",
    "                'facility_name': f'{county_name} FAC',\n",
    "                'first_production_ts': first_production_ts,    \n",
    "                'division_id': division_id,\n",
    "                'primo_prprty': primo_prprty,\n",
    "                'route_id': route_id\n",
    "            })\n",
    "\n",
    "        start_index = (start_index + batch_size) % county_row_count\n",
    "\n",
    "\n",
    "    facilities_df = pl.DataFrame(facilities)\n",
    "    return facilities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "timeseries_query = \"\"\"--sql \n",
    "    SELECT *\n",
    "    FROM await PG.insert(tank_timeseries_with_value2(), \"timeseries_dba.timeseries_data\").timeseries_data\n",
    "\"\"\"\n",
    "\n",
    "pp = await PG.fetch(timeseries_query)\n",
    "if pp is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if pp.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poop():\n",
    "    \n",
    "    # Generate the time series data with Perlin noise\n",
    "    points_per_day = 24\n",
    "    days = 2\n",
    "    hours = points_per_day * days\n",
    "\n",
    "    # Use the fetched DataFrame for time series generation\n",
    "    initial_df = pp\n",
    "    noise_dfs = map(\n",
    "        lambda x: create_ts_df_for_min(initial_df, x, hours), range(hours)\n",
    "    )\n",
    "\n",
    "    return noise_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import timedelta\n",
    "from perlin_noise import PerlinNoise\n",
    "\n",
    "# Initialize Perlin noise generator\n",
    "random_noise = PerlinNoise(octaves=1)\n",
    "\n",
    "def create_ts_df_for_min(initial_df: pl.DataFrame, hour: int, total_minutes: int) -> pl.DataFrame:\n",
    "    offset = timedelta(hours=hour)\n",
    "    # Generate noise based on timestamp and hour\n",
    "    noise = random_noise.noise(hour / total_minutes)\n",
    "    \n",
    "    # Apply noise to the DataFrame\n",
    "    noisy_df = initial_df.with_columns(\n",
    "        [\n",
    "            pl.col(\"ts\") - offset,\n",
    "            (pl.col(\"value\") + noise * 100).alias(\"value\")  # Adjust noise magnitude as needed\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return noisy_df\n",
    "\n",
    "def poop():\n",
    "    # Define parameters\n",
    "    points_per_day = 24\n",
    "    days = 2\n",
    "    hours = points_per_day * days\n",
    "    \n",
    "    # Use the fetched DataFrame for time series generation\n",
    "    initial_df = pp  # Ensure pp is a Polars DataFrame\n",
    "    \n",
    "    if not isinstance(initial_df, pl.DataFrame):\n",
    "        raise TypeError(\"pp should be a Polars DataFrame.\")\n",
    "    \n",
    "    # Generate noise for each hour\n",
    "    noise_dfs = [create_ts_df_for_min(initial_df, x, hours) for x in range(hours)]\n",
    "    \n",
    "    # Combine all generated data into one DataFrame\n",
    "    combined_df = pl.concat(noise_dfs)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new tank timeseries generation\n",
    "\n",
    "from src.pool import PG \n",
    "\n",
    "DATA_CATALOG_QUERY = \"\"\"--sql\n",
    "    SELECT *\n",
    "    FROM timeseries_dba.data_catalog AS dc \n",
    "    JOIN timeseries_dba.timeseries_data AS td ON td.key_metric = dc.key_metric\n",
    "    WHERE dc.metric_nice_name NOT LIKE '%TankID%'\n",
    "\"\"\"\n",
    "\n",
    "dc = await PG.fetch(DATA_CATALOG_QUERY)\n",
    "if dc is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if dc.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pool import PG\n",
    "\n",
    "DATA_CATALOG_QUERY = \"\"\"--sql \n",
    "    SELECT *\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "\"\"\"\n",
    "\n",
    "dc_df = await PG.fetch(DATA_CATALOG_QUERY)\n",
    "if dc_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if dc_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "dc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from src.pool import PG\n",
    "\n",
    "NEW_TANKS_QUERY = \"\"\"--sql \n",
    "    SELECT DISTINCT source_key, division_id, source_id\n",
    "    FROM timeseries_dba.data_catalog dc\n",
    "    WHERE source_id NOT IN ('69419', '98750', '98743', '480001', '110009')\n",
    "    AND source_key NOT LIKE '%FAC%'\n",
    "    ORDER BY source_key;\n",
    "\"\"\"\n",
    "\n",
    "new_tanks_df = await PG.fetch(NEW_TANKS_QUERY)\n",
    "if new_tanks_df is None:\n",
    "    raise ValueError(\"df is None\")\n",
    "if new_tanks_df.shape[0] == 0:\n",
    "        raise ValueError(\"The fetched DataFrame is empty.\")\n",
    "\n",
    "new_tanks_df\n",
    "pattern = r'0\\d'\n",
    "oil_tanks_df = new_tanks_df.filter(pl.col(\"source_key\").str.contains(pattern)) \n",
    "new_oil_df = oil_tanks_df.group_by(\"source_id\").agg(pl.count(\"source_key\").alias(\"oil_tank_count\"))\n",
    "new_oil_df = new_oil_df.sort(\"source_id\")\n",
    "new_oil_df\n",
    "water_tanks_df = new_tanks_df.filter(pl.col(\"source_key\").str.contains(\"W\"))\n",
    "new_df = water_tanks_df.group_by(\"source_id\").agg(pl.count(\"source_key\").alias(\"water_tank_count\"))\n",
    "new_df = new_df.sort(\"source_id\")\n",
    "new_df\n",
    "combined_count_df = new_df.join(new_oil_df, on=\"source_id\")\n",
    "combined_count_df\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(2000)\n",
    "def select_rand_oil(new_df: pl.DataFrame, key_metric):\n",
    "    oil_count = new_df.with_columns(\"oil_tank_count\").item()\n",
    "    return oil_count\n",
    "new_new_df = dc_df.join(combined_count_df, on=\"source_id\")\n",
    "new_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def tank_timeseries_with_value3() -> pl.DataFrame:\n",
    "    time_series = []\n",
    "\n",
    "    for tank in new_new_df.iter_rows(named=True):\n",
    "        key_metric = tank[\"key_metric\"]\n",
    "        primo_id = tank[\"source_id\"]\n",
    "        identifier = tank[\"metric_nice_name\"]\n",
    "\n",
    "        if \"Inches\" in identifier:\n",
    "            value = random.uniform(60, 400)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Define the start_date as one week ago and end_date as the current date\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=7)  # Set the start date to 1 week ago\n",
    "\n",
    "        # Generate a random timestamp between start_date and end_date\n",
    "        create_ts = start_date + (end_date - start_date) * random.random()\n",
    "\n",
    "        time_series.append({\n",
    "            \"key_metric\": key_metric,\n",
    "            \"ts\": create_ts,\n",
    "            \"value\": value\n",
    "        })\n",
    "\n",
    "    time_series_df = pl.DataFrame(time_series)\n",
    "    return time_series_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_timeseries_with_value3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
